{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a hadronization net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rnd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Layer, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data from HadronizationPrep.ipynb\n",
    "x_raw = np.loadtxt('x_raw.dat')\n",
    "y_raw = np.loadtxt('y_raw.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocess (norm everything to E1)\n",
    "for i, x in enumerate(x_raw):\n",
    "    E1 = x_raw[i,0]\n",
    "    E2 = x_raw[i,4]\n",
    "    x_raw[i,:8] = x_raw[i,:8]/(E1+E2)\n",
    "    y_raw[i,1:4] = y_raw[i,1:4]/(E1+E2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  1.23860298e-04, -3.87150854e-03,\n",
       "        -8.00250497e-01,  2.11000000e+02],\n",
       "       [ 1.00000000e+00, -1.69576004e-02,  2.18929668e-02,\n",
       "        -9.99483254e-01,  3.11000000e+02],\n",
       "       [-1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00],\n",
       "       ...,\n",
       "       [ 1.00000000e+00, -1.19333048e-04,  1.10517411e-04,\n",
       "         5.31168608e-01,  2.23000000e+02],\n",
       "       [ 1.00000000e+00,  1.17364570e-03,  5.09638066e-05,\n",
       "         8.85231046e-01,  2.21200000e+03],\n",
       "       [ 1.00000000e+00, -4.01312924e-04,  2.28374011e-03,\n",
       "         9.99996333e-01,  2.11000000e+02]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a permanent dropout layer designed to add a lot of randomness\n",
    "class PermaDropout(Layer):\n",
    "    def __init__(self, rate):\n",
    "        super(PermaDropout, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.dropout(inputs, rate=self.rate)\n",
    "\n",
    "# this is the definition of our custom loss function\n",
    "REG_LQCD = 1.0 \n",
    "REG_LQCD_SQ = REG_LQCD**2\n",
    "REG_TENS = K.constant(REG_LQCD_SQ,shape=3)\n",
    "REG_TENS4 = K.constant(REG_LQCD_SQ,shape=4)\n",
    "BATCH_SIZE = 256\n",
    "REG_TENSB4 = K.constant(REG_LQCD_SQ,shape=[BATCH_SIZE,4])\n",
    "\n",
    "# Create a loss function that adds the MSE loss to the mean of all squared activations of a specific layer\n",
    "# note: definition is asymmetric in penalizing large momenta\n",
    "def lossx(y_true,y_pred):\n",
    "    #return 1.-y_true[0]*K.tanh(y_pred[0]) + 0.5*(1.+y_true[0])*K.mean(K.square(y_pred[1:] - y_true[1:])/(K.square(y_true[1:])+REG_TENS), axis=-1)\n",
    "    return 1.-y_true[0]*K.tanh(y_pred[0]) + 0.5*(1.+y_true[0])*K.mean([0,1,1,1]*K.square(y_pred - y_true)/(K.square(y_true)+REG_TENS4), axis=-1)\n",
    "        \n",
    "        \n",
    "def cust_lossx():    \n",
    "    def lossx(y_true,y_pred):\n",
    "        A = y_true[:,0]\n",
    "        return 1.-A*K.tanh(y_pred[:,0]) + 0.5*(1.+A)*K.mean([0,1,1,1]*K.square(y_pred - y_true)/(K.square(y_true)+REG_TENSB4), axis=-1)\n",
    "\n",
    "    return lossx\n",
    "\n",
    "# use cross product\n",
    "def epsijkchoose(i,j,k):\n",
    "    if i == j or j == k or k == i:\n",
    "        return 0\n",
    "    if i == 0:\n",
    "        if j == 1:\n",
    "            return 1\n",
    "        if j == 2:\n",
    "            return -1\n",
    "    if i == 1:\n",
    "        if j == 2:\n",
    "            return 1\n",
    "        if j == 0:\n",
    "            return -1\n",
    "    if i == 2:\n",
    "        if j == 0:\n",
    "            return 1\n",
    "        if j == 1:\n",
    "            return -1\n",
    "epsijk = np.array([ [ [ epsijkchoose(i,j,k) for k in range(3)] for j in range(3)] for i in range(3)]).astype(np.float32)\n",
    "\n",
    "def cross(x,y):\n",
    "    x = x[:,-3:]\n",
    "    y = y[:,-3:]\n",
    "    epsijkbatch = K.expand_dims(EPS_IJK,len(x),axis=0)\n",
    "    epsijkbatch = K.batch_dot(epsijkbatch, x, axes=(2, 1))\n",
    "    epsijkbatch = K.batch_dot(epsijkbatch, y, axes=(2, 1))\n",
    "    return epsijkbatch\n",
    "\n",
    "def normcross(x,y):\n",
    "    return K.sqrt(K.sum(K.square(cross(x,y))))\n",
    "\n",
    "def normcross(x,y):\n",
    "    lx = x[:,2]*y[:,3] - x[:,3]*y[:,2]\n",
    "    ly = x[:,3]*y[:,1] - x[:,1]*y[:,3]\n",
    "    lz = x[:,1]*y[:,2] - x[:,2]*y[:,1]\n",
    "    lsq = np.array(lx*lx + ly*ly + lz*lz)\n",
    "    return lsq**0.5\n",
    "\n",
    "def lprod(x,y,i,j):\n",
    "    return K.batch_dot(x[:,i:i+1],y[:,j:j+1]) - K.batch_dot(x[:,j:j+1],y[:,i:i+1])\n",
    "\n",
    "def normcross(x,y):\n",
    "    lx = lprod(x,y,2,3)\n",
    "    ly = lprod(x,y,3,1)\n",
    "    lz = lprod(x,y,1,2)\n",
    "    lsq = K.sqrt(K.square(lx) + K.square(ly) + K.square(lz))\n",
    "    return K.flatten(lsq)\n",
    "\n",
    "def rtdot(x,y):\n",
    "    xdysqrt = K.sqrt(K.batch_dot(y[:,1:4]-x[:,1:4],y[:,1:4]))\n",
    "    return K.flatten(xdysqrt)\n",
    "\n",
    "def normdot(x,y):\n",
    "    xdy = K.batch_dot(y[:,1:4]-x[:,1:4],y[:,1:4])\n",
    "    return K.flatten(xdy)\n",
    "\n",
    "def lossx(y_true,y_pred):\n",
    "    A = y_true[:,0]\n",
    "    B = 3*A - 2\n",
    "    yabs = K.sqrt(K.mean([0,1,1,1]*K.square(y_true), axis=-1)+1e-14)\n",
    "    return 1.-B*K.tanh(y_pred[:,0]) + 0.5*(1.+A)/yabs*(K.mean([0,1,1,1]*K.square(y_pred - y_true), axis=-1) + normdot(y_pred, y_true) + normcross(y_pred, y_true))\n",
    "\n",
    "\n",
    "def cust_lossx():    \n",
    "    def lossx(y_true,y_pred):\n",
    "        A = y_true[:,0]\n",
    "        if A == -1:\n",
    "            A = -10\n",
    "        return 1.-A*K.tanh(y_pred[:,0]) + 0.5*(1.+A)*(K.mean([0,1,1,1]*K.square(y_pred - y_true), axis=-1) + normcross(y_pred, y_true))\n",
    "\n",
    "    return lossx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.5], shape=(1,), dtype=float64)\n",
      "tf.Tensor([0.0057064], shape=(1,), dtype=float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=555644, shape=(1,), dtype=float64, numpy=array([6.])>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(normdot(np.array([[1,0.0005,0.0005,0.5]]),np.array([[1,0.0005,0.0005,-0.5]])))\n",
    "print(normcross(np.array([[1,0.005,0.005,0.5]]),np.array([[1,0.0005,0.005,-0.5]])))\n",
    "lossx(np.array([[-1,0.005,0.005,0.5]]),np.array([[100,0.0005,0.005,-0.5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513792 60416 30208\n"
     ]
    }
   ],
   "source": [
    "TRAINCUT = int((int(0.85*len(x_raw))//BATCH_SIZE) * BATCH_SIZE)\n",
    "VALCUT = int((int(0.1*len(x_raw))//BATCH_SIZE) * BATCH_SIZE)\n",
    "TESTCUT = int((int(0.05*len(x_raw))//BATCH_SIZE) * BATCH_SIZE)\n",
    "print(\"{} {} {}\".format(TRAINCUT,VALCUT,TESTCUT))\n",
    "\n",
    "# Note with this data, I am assuming it doesn't need to be shuffled, but may wish to do that later\n",
    "\n",
    "x_train = x_raw[:TRAINCUT,:8]\n",
    "y_train = y_raw[:TRAINCUT,:4]\n",
    "\n",
    "x_val = x_raw[TRAINCUT:TRAINCUT+VALCUT,:8]\n",
    "y_val = y_raw[TRAINCUT:TRAINCUT+VALCUT,:4]\n",
    "\n",
    "x_test = x_raw[-TESTCUT:,:8]\n",
    "y_test = y_raw[-TESTCUT:,:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarAutoEncoder():\n",
    "    # feature extractor model\n",
    "    def __init__(self):\n",
    "        self.z_part12 = 8 \n",
    "        self.z_part3 = 4 \n",
    "        self.z_dim = 3 \n",
    "        self._build()\n",
    "    \n",
    "    def _build(self):\n",
    "        part12_input = Input(shape=(self.z_part12,), name='part12_input')\n",
    "        part3_input = Input(shape=(self.z_part3,), name='part3_input')\n",
    "        \n",
    "        # encoder\n",
    "        enc_cat = Concatenate()([part3_input, part12_input])\n",
    "        enc = Dense(32, activation='relu')(enc_cat)\n",
    "        enc = Dense(32, activation='relu')(enc)\n",
    "        enc = Dense(32, activation='relu')(enc)\n",
    "        # construct variational aspect\n",
    "        self.mu = Dense(self.z_dim, name='mu')(enc)\n",
    "        self.log_var = Dense(self.z_dim, name='log_var')(enc)\n",
    "        #enc_outprep = Concatenate()([self.mu, self.log_var])\n",
    "        #self.encoder_mu_log_var = Model([part3_input,part12_input], [self.mu, self.log_var])\n",
    "        \n",
    "        def sampling(args):\n",
    "            mu, log_var = args\n",
    "            epsilon = K.random_normal(shape=K.shape(mu), mean=0., stddev=1.)\n",
    "            return mu + K.exp(log_var / 2) * epsilon\n",
    "\n",
    "        enc_output = Lambda(sampling, name='encoder_output')([self.mu, self.log_var])\n",
    "        self.encoder = Model([part3_input,part12_input],enc_output)\n",
    "        \n",
    "        # decoder\n",
    "        encoded_input = Input(shape=(self.z_dim,), name='enc_part3_input')\n",
    "        dec_input = Concatenate()([encoded_input, part12_input])\n",
    "        dec = Dense(32, activation='relu')(dec_input)\n",
    "        #PermaDropout(0.5),\n",
    "        dec = Dense(32, activation='relu')(dec)\n",
    "        #PermaDropout(0.5),\n",
    "        dec = Dense(32, activation='relu')(dec)\n",
    "        #PermaDropout(0.5),\n",
    "        dec_output = Dense(self.z_part3, activation='linear')(dec)\n",
    "        self.decoder = Model([encoded_input, part12_input], dec_output)\n",
    "        \n",
    "        # the autoencoder\n",
    "        model_input = [part3_input,part12_input]\n",
    "        model_output = self.decoder([enc_output,part12_input])\n",
    "\n",
    "        self.autoencoder = Model(model_input, model_output)\n",
    "    \n",
    "    def compile(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        optimizer = Adam(lr=learning_rate)\n",
    "        \n",
    "        def vae_kl_loss(y_true, y_pred):\n",
    "            kl_loss =  -0.5 * K.sum(1 + self.log_var - K.square(self.mu) - K.exp(self.log_var), axis = 1)\n",
    "            return kl_loss\n",
    "        \n",
    "        def lossx(y_true,y_pred, emitScale = 1.5):\n",
    "            A = y_true[:,0]\n",
    "            \"\"\" Note: This scale encourages the program to get the emission prob correct.  \n",
    "                If too small (e.g. 1), it is never favorable.  If too large, e.g. 5 it is always favorable,\n",
    "                but at the cost of the other part of the loss.\n",
    "            \"\"\"\n",
    "            B = emitScale*A - (emitScale-1)\n",
    "            yabs = K.sqrt(K.mean([0,1,1,1]*K.square(y_true), axis=-1)+1e-14)\n",
    "            return 1.-B*K.tanh(y_pred[:,0]) + 0.5*(1.+A)/yabs*(K.mean([0,1,1,1]*K.square(y_pred - y_true), axis=-1) + normdot(y_pred, y_true) + normcross(y_pred, y_true))\n",
    "        \n",
    "        def vae_loss(y_true, y_pred, scale = 1):\n",
    "            loss_x = lossx(y_true, y_pred)\n",
    "            kl_loss = vae_kl_loss(y_true, y_pred)\n",
    "            return  scale*loss_x + kl_loss\n",
    "        \n",
    "        \"\"\" Here, I need to set experimental_run_tf_function=False in order for this to execute.\n",
    "            I do not know why exactly, but with a TF2 upgrade this may go away. See discussion:\n",
    "            https://github.com/tensorflow/probability/issues/519\n",
    "        \"\"\"\n",
    "        self.autoencoder.compile(optimizer=optimizer, loss = vae_loss, metrics = [vae_kl_loss, lossx],\n",
    "                                experimental_run_tf_function=False)\n",
    "        \n",
    "    def train(self, x_train, y_train, batch_size, epochs, validation_data=None):\n",
    "        \n",
    "        if validation_data is not None:\n",
    "            x_val = validation_data[0]\n",
    "            y_val = validation_data[1]\n",
    "            validation_data=[[y_val,x_val],y_val]\n",
    "\n",
    "        self.autoencoder.fit([y_train, x_train], \n",
    "                             y_train,\n",
    "                             batch_size = batch_size,\n",
    "                             shuffle = True,\n",
    "                             epochs = epochs,\n",
    "                             validation_data=validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_auto_encoder = VarAutoEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "var_auto_encoder.compile(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 513792 samples, validate on 60416 samples\n",
      "Epoch 1/15\n",
      "513792/513792 [==============================] - 40s 79us/sample - loss: 0.9568 - vae_kl_loss: 0.0490 - lossx: 0.9078 - val_loss: 0.6640 - val_vae_kl_loss: 0.0882 - val_lossx: 0.5758\n",
      "Epoch 2/15\n",
      "513792/513792 [==============================] - 35s 69us/sample - loss: 0.6484 - vae_kl_loss: 0.0978 - lossx: 0.5505 - val_loss: 0.6115 - val_vae_kl_loss: 0.0929 - val_lossx: 0.5186\n",
      "Epoch 3/15\n",
      "449152/513792 [=========================>....] - ETA: 4s - loss: 0.5931 - vae_kl_loss: 0.1025 - lossx: 0.4906"
     ]
    }
   ],
   "source": [
    "var_auto_encoder.train(x_train, y_train, 128, 15, validation_data=[x_val,y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.73546911  0.04295983  0.01637338  0.7340304   0.26453089 -0.04474383\n",
      "   0.14407936  0.21729179]]\n",
      "[[-1.  0.  0.  0.]]\n",
      "[[-0.4313089   0.34734607  2.5874135 ]]\n",
      "[[2.8373638e+01 2.5901036e-02 8.8629372e-02 1.1157228e+00]]\n",
      "[[-1.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "idx = 31\n",
    "xs = x_train[idx:idx+1]\n",
    "ys = y_train[idx:idx+1]\n",
    "enc_out = var_auto_encoder.encoder.predict([ys,xs])\n",
    "dec_out = var_auto_encoder.decoder.predict([enc_out,xs])\n",
    "print(xs)\n",
    "print(ys)\n",
    "print(enc_out)\n",
    "print(dec_out)\n",
    "print(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder():\n",
    "    # feature extractor model\n",
    "    def __init__(self):\n",
    "        self.z_part12 = 8 \n",
    "        self.z_part3 = 4 \n",
    "        self.z_dim = 3 \n",
    "        self._build()\n",
    "    \n",
    "    def _build(self):\n",
    "        part12_input = Input(shape=(self.z_part12,), name='part12_input')\n",
    "        part3_input = Input(shape=(self.z_part3,), name='part3_input')\n",
    "        \n",
    "        # encoder\n",
    "        enc_cat = Concatenate()([part3_input, part12_input])\n",
    "        enc = Dense(32, activation='relu')(enc_cat)\n",
    "        enc = Dense(32, activation='relu')(enc)\n",
    "        enc = Dense(32, activation='relu')(enc)\n",
    "        encoded = Dense(self.z_dim, activation='linear')(enc)\n",
    "        self.encoder = Model([part3_input,part12_input],encoded)\n",
    "        \n",
    "        # decoder\n",
    "        encoded_input = Input(shape=(self.z_dim,), name='enc_part3_input')\n",
    "        dec_input = Concatenate()([encoded_input, part12_input])\n",
    "        dec = Dense(64, activation='relu')(dec_input)\n",
    "        #PermaDropout(0.5),\n",
    "        dec = Dense(64, activation='relu')(dec)\n",
    "        #PermaDropout(0.5),\n",
    "        dec = Dense(64, activation='relu')(dec)\n",
    "        #PermaDropout(0.5),\n",
    "        dec_output = Dense(self.z_part3, activation='linear')(dec)\n",
    "        self.decoder = Model([encoded_input, part12_input], dec_output)\n",
    "        \n",
    "        # the autoencoder\n",
    "        model_input = [part3_input,part12_input]\n",
    "        model_output = self.decoder([encoded,part12_input])\n",
    "\n",
    "        self.autoencoder = Model(model_input, model_output)\n",
    "    \n",
    "    def compile(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        optimizer = Adam(lr=learning_rate)\n",
    "        \n",
    "        def lossx(y_true,y_pred):\n",
    "            A = y_true[:,0]\n",
    "            yabs = K.sqrt(K.mean([0,1,1,1]*K.square(y_true), axis=-1)+1e-14)\n",
    "            return 1.-A*K.tanh(y_pred[:,0]) + 0.5*(1.+A)/yabs*(K.mean([0,1,1,1]*K.square(y_pred - y_true), axis=-1) + normdot(y_pred, y_true) + normcross(y_pred, y_true))\n",
    "        \n",
    "        self.autoencoder.compile(optimizer=optimizer, loss = lossx)\n",
    "        \n",
    "    def train(self, x_train, y_train, batch_size, epochs, validation_data=None):\n",
    "        \n",
    "        if validation_data is not None:\n",
    "            x_val = validation_data[0]\n",
    "            y_val = validation_data[1]\n",
    "            validation_data=[[y_val,x_val],y_val]\n",
    "\n",
    "        self.autoencoder.fit([y_train, x_train], \n",
    "                             y_train,\n",
    "                             batch_size = batch_size,\n",
    "                             shuffle = True,\n",
    "                             epochs = epochs,\n",
    "                             validation_data=validation_data\n",
    "                            \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder = AutoEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "auto_encoder.compile(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 513792 samples, validate on 60416 samples\n",
      "Epoch 1/15\n",
      "513792/513792 [==============================] - 30s 59us/sample - loss: 1.0019 - val_loss: 0.7041\n",
      "Epoch 2/15\n",
      "513792/513792 [==============================] - 32s 62us/sample - loss: 0.5596 - val_loss: 0.3340\n",
      "Epoch 3/15\n",
      "513792/513792 [==============================] - 30s 59us/sample - loss: 0.4461 - val_loss: 0.3782\n",
      "Epoch 4/15\n",
      "513792/513792 [==============================] - 30s 58us/sample - loss: 0.3584 - val_loss: 0.8633\n",
      "Epoch 5/15\n",
      "513792/513792 [==============================] - 30s 58us/sample - loss: 0.3065 - val_loss: 0.4606\n",
      "Epoch 6/15\n",
      "513792/513792 [==============================] - 30s 58us/sample - loss: 0.2541 - val_loss: 0.2671\n",
      "Epoch 7/15\n",
      "513792/513792 [==============================] - 30s 59us/sample - loss: 0.2281 - val_loss: 0.2101\n",
      "Epoch 8/15\n",
      "513792/513792 [==============================] - 30s 58us/sample - loss: 0.2937 - val_loss: 0.3135\n",
      "Epoch 9/15\n",
      "513792/513792 [==============================] - 30s 59us/sample - loss: 0.2627 - val_loss: 0.2178\n",
      "Epoch 10/15\n",
      "513792/513792 [==============================] - 31s 60us/sample - loss: 0.2456 - val_loss: 0.1666\n",
      "Epoch 11/15\n",
      "513792/513792 [==============================] - 30s 58us/sample - loss: 0.2222 - val_loss: 0.2418\n",
      "Epoch 12/15\n",
      "513792/513792 [==============================] - 32s 62us/sample - loss: 0.2009 - val_loss: 0.1255\n",
      "Epoch 13/15\n",
      "513792/513792 [==============================] - 30s 59us/sample - loss: 0.2591 - val_loss: 0.1000\n",
      "Epoch 14/15\n",
      "513792/513792 [==============================] - 31s 61us/sample - loss: 0.2038 - val_loss: 0.0914\n",
      "Epoch 15/15\n",
      "513792/513792 [==============================] - 32s 62us/sample - loss: 0.2197 - val_loss: 0.2191\n"
     ]
    }
   ],
   "source": [
    "auto_encoder.train(x_train, y_train, 128, 15, validation_data=[x_val,y_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.59124529  0.00103248  0.00326767  0.59123535  0.40875471 -0.01086593\n",
      "  -0.00165995  0.40860688]]\n",
      "[[1.00000000e+00 6.57285212e-04 4.81170271e-03 4.01718718e-01]]\n",
      "[[30.588957  -1.5729476  2.5272653]]\n",
      "[[3.6106445e+01 1.2611160e-03 5.5991746e-03 4.3408090e-01]]\n",
      "[[1.00000000e+00 6.57285212e-04 4.81170271e-03 4.01718718e-01]]\n"
     ]
    }
   ],
   "source": [
    "idx = 22\n",
    "xs = x_train[idx:idx+1]\n",
    "ys = y_train[idx:idx+1]\n",
    "enc_out = auto_encoder.encoder.predict([ys,xs])\n",
    "dec_out = auto_encoder.decoder.predict([enc_out,xs])\n",
    "print(xs)\n",
    "print(ys)\n",
    "print(enc_out)\n",
    "print(dec_out)\n",
    "print(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial momentum model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_momentum_model():\n",
    "    # feature extractor model\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu',input_shape=[8]),\n",
    "        PermaDropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        PermaDropout(0.5),\n",
    "        Dense(4, activation='linear')\n",
    "        ])\n",
    "    print(model.summary())\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',loss=cust_lossx())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_27 (Dense)             (None, 64)                576       \n",
      "_________________________________________________________________\n",
      "perma_dropout_18 (PermaDropo (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "perma_dropout_19 (PermaDropo (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 4,996\n",
      "Trainable params: 4,996\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = define_momentum_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training momentum model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 513792 samples, validate on 60416 samples\n",
      "Epoch 1/30\n",
      "513792/513792 [==============================] - 24s 48us/sample - loss: 0.3210 - val_loss: 0.3052\n",
      "Epoch 2/30\n",
      "513792/513792 [==============================] - 21s 42us/sample - loss: 0.3029 - val_loss: 0.3034\n",
      "Epoch 3/30\n",
      "513792/513792 [==============================] - 22s 43us/sample - loss: 0.3019 - val_loss: 0.3030\n",
      "Epoch 4/30\n",
      "513792/513792 [==============================] - 23s 44us/sample - loss: 0.3015 - val_loss: 0.3024\n",
      "Epoch 5/30\n",
      "513792/513792 [==============================] - 21s 42us/sample - loss: 0.3012 - val_loss: 0.3023\n",
      "Epoch 6/30\n",
      " 17152/513792 [>.............................] - ETA: 20s - loss: 0.2986"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-b3ae29e37131>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=30, batch_size = BATCH_SIZE, validation_data=[x_val,y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.18513115 -0.00340915  0.00204126  0.18502525  0.81486885  0.00235271\n",
      "  0.00728108 -0.81481431]\n",
      "[ 1.00000000e+00 -7.78584587e-04  3.80717898e-03  1.37925427e-01]\n",
      "[[6.6870565e+00 6.9317431e-04 1.5475576e-04 1.5962045e-03]]\n"
     ]
    }
   ],
   "source": [
    "idx = 202\n",
    "print(x_train[idx])\n",
    "print(y_train[idx])\n",
    "print(model.predict(x_train[idx:idx+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.46594976e-01 -1.42339427e-01  4.69652409e-03  5.27704385e-01\n",
      "  4.53405024e-01 -3.32098278e-01  5.41031581e-04 -3.08685196e-01]\n",
      "[-1.  0.  0.  0.]\n",
      "[[9.3255873e+00 8.0299063e-04 1.2794735e-03 6.3834339e-02]]\n"
     ]
    }
   ],
   "source": [
    "idx = 102\n",
    "print(x_train[idx])\n",
    "print(y_train[idx])\n",
    "print(model.predict(x_train[idx:idx+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flavor model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513920 60416 30208\n"
     ]
    }
   ],
   "source": [
    "TRAINCUT = int((int(0.85*len(x_raw))//BATCH_SIZE) * BATCH_SIZE)\n",
    "VALCUT = int((int(0.1*len(x_raw))//BATCH_SIZE) * BATCH_SIZE)\n",
    "TESTCUT = int((int(0.05*len(x_raw))//BATCH_SIZE) * BATCH_SIZE)\n",
    "print(\"{} {} {}\".format(TRAINCUT,VALCUT,TESTCUT))\n",
    "\n",
    "# Note with this data, I am assuming it doesn't need to be shuffled, but may wish to do that later\n",
    "\n",
    "encin = OneHotEncoder(sparse = False)\n",
    "xf_ohe = encin.fit_transform(x_raw[:,8:].astype(int))\n",
    "\n",
    "encout = OneHotEncoder(sparse = False)\n",
    "yf_ohe = encout.fit_transform(y_raw[:,4:].astype(int))\n",
    "\n",
    "xf_train = xf_ohe[:TRAINCUT]\n",
    "yf_train = yf_ohe[:TRAINCUT]\n",
    "\n",
    "xf_val = xf_ohe[TRAINCUT:TRAINCUT+VALCUT]\n",
    "yf_val = yf_ohe[TRAINCUT:TRAINCUT+VALCUT]\n",
    "\n",
    "xf_test = xf_ohe[-TESTCUT:]\n",
    "yf_test = yf_ohe[-TESTCUT:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "nIn = xf_ohe.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(513920, 89)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yf_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflavorset = [ 21, 1,-1,2,-2,3,-3 ]\n",
    "outflavorset = [ ]\n",
    "for x in x_raw[:,8:]:\n",
    "    for y in x:\n",
    "        if y not in inflavorset:\n",
    "            inflavorset.append(y)\n",
    "\n",
    "for y in y_raw[:,4]:\n",
    "    if y not in outflavorset:\n",
    "        outflavorset.append(y)\n",
    "\n",
    "nInStates = len(inflavorset)\n",
    "nOutStates = len(outflavorset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_flavor_model():\n",
    "    # feature extractor model\n",
    "    inputs = Input(shape=(nIn,))\n",
    "    l1 = Dense(64, activation='relu')(inputs)\n",
    "    l2 = Dense(64, activation='relu')(l1)\n",
    "    outputs = Dense(nOutStates, activation='softmax')(l2)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    print(model.summary())\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 39)]              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                2560      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 89)                5785      \n",
      "=================================================================\n",
      "Total params: 12,505\n",
      "Trainable params: 12,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "flavormodel=define_flavor_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 513920 samples, validate on 60416 samples\n",
      "Epoch 1/30\n",
      "513920/513920 [==============================] - 29s 56us/sample - loss: 2.2018 - val_loss: 2.1454\n",
      "Epoch 2/30\n",
      "513920/513920 [==============================] - 30s 58us/sample - loss: 2.1457 - val_loss: 2.1407\n",
      "Epoch 3/30\n",
      "513920/513920 [==============================] - 36s 70us/sample - loss: 2.1427 - val_loss: 2.1407\n",
      "Epoch 4/30\n",
      "513920/513920 [==============================] - 32s 63us/sample - loss: 2.1409 - val_loss: 2.1375\n",
      "Epoch 5/30\n",
      "513920/513920 [==============================] - 32s 62us/sample - loss: 2.1392 - val_loss: 2.1378\n",
      "Epoch 6/30\n",
      "513920/513920 [==============================] - 31s 60us/sample - loss: 2.1378 - val_loss: 2.1378\n",
      "Epoch 7/30\n",
      "513920/513920 [==============================] - 28s 55us/sample - loss: 2.1368 - val_loss: 2.1365\n",
      "Epoch 8/30\n",
      "513920/513920 [==============================] - 32s 61us/sample - loss: 2.1363 - val_loss: 2.1366\n",
      "Epoch 9/30\n",
      "513920/513920 [==============================] - 32s 62us/sample - loss: 2.1356 - val_loss: 2.1349\n",
      "Epoch 10/30\n",
      "513920/513920 [==============================] - 32s 61us/sample - loss: 2.1352 - val_loss: 2.1355\n",
      "Epoch 11/30\n",
      "513920/513920 [==============================] - 30s 59us/sample - loss: 2.1349 - val_loss: 2.1352\n",
      "Epoch 12/30\n",
      "513920/513920 [==============================] - 31s 60us/sample - loss: 2.1346 - val_loss: 2.1355\n",
      "Epoch 13/30\n",
      "513920/513920 [==============================] - 31s 60us/sample - loss: 2.1343 - val_loss: 2.1345\n",
      "Epoch 14/30\n",
      "513920/513920 [==============================] - 31s 61us/sample - loss: 2.1341 - val_loss: 2.1360\n",
      "Epoch 15/30\n",
      "513920/513920 [==============================] - 36s 71us/sample - loss: 2.1339 - val_loss: 2.1354\n",
      "Epoch 16/30\n",
      "513920/513920 [==============================] - 30s 58us/sample - loss: 2.1337 - val_loss: 2.1355\n",
      "Epoch 17/30\n",
      "513920/513920 [==============================] - 29s 56us/sample - loss: 2.1335 - val_loss: 2.1349\n",
      "Epoch 18/30\n",
      "513920/513920 [==============================] - 28s 55us/sample - loss: 2.1334 - val_loss: 2.1345\n",
      "Epoch 19/30\n",
      "513920/513920 [==============================] - 28s 54us/sample - loss: 2.1333 - val_loss: 2.1351\n",
      "Epoch 20/30\n",
      "513920/513920 [==============================] - 28s 54us/sample - loss: 2.1331 - val_loss: 2.1350\n",
      "Epoch 21/30\n",
      "513920/513920 [==============================] - 27s 52us/sample - loss: 2.1331 - val_loss: 2.1355\n",
      "Epoch 22/30\n",
      "513920/513920 [==============================] - 26s 51us/sample - loss: 2.1330 - val_loss: 2.1355\n",
      "Epoch 23/30\n",
      "513920/513920 [==============================] - 29s 57us/sample - loss: 2.1329 - val_loss: 2.1360\n",
      "Epoch 24/30\n",
      "513920/513920 [==============================] - 27s 52us/sample - loss: 2.1329 - val_loss: 2.1356\n",
      "Epoch 25/30\n",
      "513920/513920 [==============================] - 29s 57us/sample - loss: 2.1327 - val_loss: 2.1353\n",
      "Epoch 26/30\n",
      "513920/513920 [==============================] - 28s 54us/sample - loss: 2.1327 - val_loss: 2.1351\n",
      "Epoch 27/30\n",
      "513920/513920 [==============================] - 32s 62us/sample - loss: 2.1326 - val_loss: 2.1359\n",
      "Epoch 28/30\n",
      "513920/513920 [==============================] - 28s 54us/sample - loss: 2.1325 - val_loss: 2.1355\n",
      "Epoch 29/30\n",
      "513920/513920 [==============================] - 28s 55us/sample - loss: 2.1325 - val_loss: 2.1367\n",
      "Epoch 30/30\n",
      "513920/513920 [==============================] - 29s 56us/sample - loss: 2.1324 - val_loss: 2.1370\n"
     ]
    }
   ],
   "source": [
    "history = flavormodel.fit(xf_train, yf_train, epochs=30, batch_size = BATCH_SIZE, validation_data=[xf_val,yf_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(513920, 89)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yf_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a hadronization net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rnd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data from HadronizationPrep.ipynb\n",
    "x_raw = np.loadtxt('x_raw.dat')\n",
    "y_raw = np.loadtxt('y_raw.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a permanent dropout layer designed to add a lot of randomness\n",
    "class PermaDropout(Layer):\n",
    "    def __init__(self, rate):\n",
    "        super(PermaDropout, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.dropout(inputs, rate=self.rate)\n",
    "\n",
    "# this is the definition of our custom loss function\n",
    "REG_LQCD = 1.0 \n",
    "REG_LQCD_SQ = REG_LQCD**2\n",
    "REG_TENS = K.constant(REG_LQCD_SQ,shape=3)\n",
    "REG_TENS4 = K.constant(REG_LQCD_SQ,shape=4)\n",
    "BATCH_SIZE = 128\n",
    "REG_TENSB4 = K.constant(REG_LQCD_SQ,shape=[BATCH_SIZE,4])\n",
    "\n",
    "# Create a loss function that adds the MSE loss to the mean of all squared activations of a specific layer\n",
    "# note: definition is asymmetric in penalizing large momenta\n",
    "def lossx(y_true,y_pred):\n",
    "    #return 1.-y_true[0]*K.tanh(y_pred[0]) + 0.5*(1.+y_true[0])*K.mean(K.square(y_pred[1:] - y_true[1:])/(K.square(y_true[1:])+REG_TENS), axis=-1)\n",
    "    return 1.-y_true[0]*K.tanh(y_pred[0]) + 0.5*(1.+y_true[0])*K.mean([0,1,1,1]*K.square(y_pred - y_true)/(K.square(y_true)+REG_TENS4), axis=-1)\n",
    "        \n",
    "        \n",
    "def cust_lossx():    \n",
    "    def lossx(y_true,y_pred):\n",
    "        A = y_true[:,0]\n",
    "        return 1.-A*K.tanh(y_pred[:,0]) + 0.5*(1.+A)*K.mean([0,1,1,1]*K.square(y_pred - y_true)/(K.square(y_true)+REG_TENSB4), axis=-1)\n",
    "\n",
    "    return lossx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_momentum_model():\n",
    "    # feature extractor model\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu',input_shape=[8]),\n",
    "        PermaDropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        PermaDropout(0.5),\n",
    "        Dense(4, activation='linear')\n",
    "        ])\n",
    "    print(model.summary())\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',loss=cust_lossx())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 64)                576       \n",
      "_________________________________________________________________\n",
      "perma_dropout_2 (PermaDropou (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "perma_dropout_3 (PermaDropou (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 4,996\n",
      "Trainable params: 4,996\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = define_momentum_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training momentum model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513920 60416 30208\n"
     ]
    }
   ],
   "source": [
    "TRAINCUT = int((int(0.85*len(x_raw))//BATCH_SIZE) * BATCH_SIZE)\n",
    "VALCUT = int((int(0.1*len(x_raw))//BATCH_SIZE) * BATCH_SIZE)\n",
    "TESTCUT = int((int(0.05*len(x_raw))//BATCH_SIZE) * BATCH_SIZE)\n",
    "print(\"{} {} {}\".format(TRAINCUT,VALCUT,TESTCUT))\n",
    "\n",
    "# Note with this data, I am assuming it doesn't need to be shuffled, but may wish to do that later\n",
    "\n",
    "x_train = x_raw[:TRAINCUT,:8]\n",
    "y_train = y_raw[:TRAINCUT,:4]\n",
    "\n",
    "x_val = x_raw[TRAINCUT:TRAINCUT+VALCUT,:8]\n",
    "y_val = y_raw[TRAINCUT:TRAINCUT+VALCUT,:4]\n",
    "\n",
    "x_test = x_raw[-TESTCUT:,:8]\n",
    "y_test = y_raw[-TESTCUT:,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 513920 samples, validate on 60416 samples\n",
      "Epoch 1/30\n",
      "513920/513920 [==============================] - 26s 50us/sample - loss: 511.7816 - val_loss: 0.9979\n",
      "Epoch 2/30\n",
      "513920/513920 [==============================] - 23s 46us/sample - loss: 0.7338 - val_loss: 0.5837\n",
      "Epoch 3/30\n",
      "513920/513920 [==============================] - 25s 48us/sample - loss: 0.5520 - val_loss: 0.5280\n",
      "Epoch 4/30\n",
      "513920/513920 [==============================] - 22s 42us/sample - loss: 0.5460 - val_loss: 0.5150\n",
      "Epoch 5/30\n",
      "513920/513920 [==============================] - 24s 47us/sample - loss: 0.5846 - val_loss: 0.5232\n",
      "Epoch 6/30\n",
      "513920/513920 [==============================] - 28s 54us/sample - loss: 0.5257 - val_loss: 0.5141\n",
      "Epoch 7/30\n",
      "513920/513920 [==============================] - 21s 40us/sample - loss: 0.5778 - val_loss: 0.5149\n",
      "Epoch 8/30\n",
      "513920/513920 [==============================] - 21s 41us/sample - loss: 0.5157 - val_loss: 0.5149\n",
      "Epoch 9/30\n",
      "513920/513920 [==============================] - 20s 40us/sample - loss: 0.9615 - val_loss: 0.5151\n",
      "Epoch 10/30\n",
      "513920/513920 [==============================] - 22s 42us/sample - loss: 0.5150 - val_loss: 0.5151\n",
      "Epoch 11/30\n",
      "513920/513920 [==============================] - 22s 42us/sample - loss: 0.5148 - val_loss: 0.5150\n",
      "Epoch 12/30\n",
      "513920/513920 [==============================] - 22s 42us/sample - loss: 1.5083 - val_loss: 0.5151\n",
      "Epoch 13/30\n",
      "513920/513920 [==============================] - 22s 43us/sample - loss: 0.5346 - val_loss: 0.5151\n",
      "Epoch 14/30\n",
      "513920/513920 [==============================] - 24s 47us/sample - loss: 0.5419 - val_loss: 0.5152\n",
      "Epoch 15/30\n",
      "513920/513920 [==============================] - 21s 42us/sample - loss: 0.5138 - val_loss: 0.5150\n",
      "Epoch 16/30\n",
      "513920/513920 [==============================] - 22s 42us/sample - loss: 0.5184 - val_loss: 0.5151\n",
      "Epoch 17/30\n",
      "513920/513920 [==============================] - 22s 42us/sample - loss: 0.5228 - val_loss: 0.5150\n",
      "Epoch 18/30\n",
      "513920/513920 [==============================] - 22s 43us/sample - loss: 0.5173 - val_loss: 0.5150\n",
      "Epoch 19/30\n",
      "513920/513920 [==============================] - 26s 51us/sample - loss: 0.5214 - val_loss: 0.5151\n",
      "Epoch 20/30\n",
      "513920/513920 [==============================] - 26s 50us/sample - loss: 0.5136 - val_loss: 0.5151\n",
      "Epoch 21/30\n",
      "513920/513920 [==============================] - 24s 47us/sample - loss: 0.5136 - val_loss: 0.5150\n",
      "Epoch 22/30\n",
      "513920/513920 [==============================] - 24s 48us/sample - loss: 0.5136 - val_loss: 0.5152\n",
      "Epoch 23/30\n",
      "513920/513920 [==============================] - 24s 46us/sample - loss: 0.5136 - val_loss: 0.5152\n",
      "Epoch 24/30\n",
      "513920/513920 [==============================] - 22s 42us/sample - loss: 0.6785 - val_loss: 0.5150\n",
      "Epoch 25/30\n",
      "513920/513920 [==============================] - 22s 43us/sample - loss: 0.5136 - val_loss: 0.5150\n",
      "Epoch 26/30\n",
      "513920/513920 [==============================] - 23s 44us/sample - loss: 0.7058 - val_loss: 0.5150\n",
      "Epoch 27/30\n",
      "513920/513920 [==============================] - 22s 42us/sample - loss: 0.5136 - val_loss: 0.5151\n",
      "Epoch 28/30\n",
      "513920/513920 [==============================] - 22s 43us/sample - loss: 0.5136 - val_loss: 0.5150\n",
      "Epoch 29/30\n",
      "513920/513920 [==============================] - 23s 45us/sample - loss: 0.5136 - val_loss: 0.5150\n",
      "Epoch 30/30\n",
      "513920/513920 [==============================] - 21s 41us/sample - loss: 0.5136 - val_loss: 0.5152\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=30, batch_size = BATCH_SIZE, validation_data=[x_val,y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.749102   -0.21227592  0.17818509  0.69593173  0.14085039 -0.10930122\n",
      "  0.06399198  0.06162063]\n",
      "[[ 9.0027885e+00  1.8762186e-02  7.3483270e-03 -5.5486590e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[100])\n",
    "print(model.predict(x_train[100:101]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flavor model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513920 60416 30208\n"
     ]
    }
   ],
   "source": [
    "TRAINCUT = int((int(0.85*len(x_raw))//BATCH_SIZE) * BATCH_SIZE)\n",
    "VALCUT = int((int(0.1*len(x_raw))//BATCH_SIZE) * BATCH_SIZE)\n",
    "TESTCUT = int((int(0.05*len(x_raw))//BATCH_SIZE) * BATCH_SIZE)\n",
    "print(\"{} {} {}\".format(TRAINCUT,VALCUT,TESTCUT))\n",
    "\n",
    "# Note with this data, I am assuming it doesn't need to be shuffled, but may wish to do that later\n",
    "\n",
    "encin = OneHotEncoder(sparse = False)\n",
    "xf_ohe = encin.fit_transform(x_raw[:,8:].astype(int))\n",
    "\n",
    "encout = OneHotEncoder(sparse = False)\n",
    "yf_ohe = encout.fit_transform(y_raw[:,4:].astype(int))\n",
    "\n",
    "xf_train = xf_ohe[:TRAINCUT]\n",
    "yf_train = yf_ohe[:TRAINCUT]\n",
    "\n",
    "xf_val = xf_ohe[TRAINCUT:TRAINCUT+VALCUT]\n",
    "yf_val = yf_ohe[TRAINCUT:TRAINCUT+VALCUT]\n",
    "\n",
    "xf_test = xf_ohe[-TESTCUT:]\n",
    "yf_test = yf_ohe[-TESTCUT:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "nIn = xf_ohe.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(513920, 89)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yf_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflavorset = [ 21, 1,-1,2,-2,3,-3 ]\n",
    "outflavorset = [ ]\n",
    "for x in x_raw[:,8:]:\n",
    "    for y in x:\n",
    "        if y not in inflavorset:\n",
    "            inflavorset.append(y)\n",
    "\n",
    "for y in y_raw[:,4]:\n",
    "    if y not in outflavorset:\n",
    "        outflavorset.append(y)\n",
    "\n",
    "nInStates = len(inflavorset)\n",
    "nOutStates = len(outflavorset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_flavor_model():\n",
    "    # feature extractor model\n",
    "    inputs = Input(shape=(nIn,))\n",
    "    l1 = Dense(64, activation='relu')(inputs)\n",
    "    l2 = Dense(64, activation='relu')(l1)\n",
    "    outputs = Dense(nOutStates, activation='softmax')(l2)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    print(model.summary())\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 39)]              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                2560      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 89)                5785      \n",
      "=================================================================\n",
      "Total params: 12,505\n",
      "Trainable params: 12,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "flavormodel=define_flavor_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 513920 samples, validate on 60416 samples\n",
      "Epoch 1/30\n",
      "513920/513920 [==============================] - 29s 56us/sample - loss: 2.2018 - val_loss: 2.1454\n",
      "Epoch 2/30\n",
      "513920/513920 [==============================] - 30s 58us/sample - loss: 2.1457 - val_loss: 2.1407\n",
      "Epoch 3/30\n",
      "513920/513920 [==============================] - 36s 70us/sample - loss: 2.1427 - val_loss: 2.1407\n",
      "Epoch 4/30\n",
      "513920/513920 [==============================] - 32s 63us/sample - loss: 2.1409 - val_loss: 2.1375\n",
      "Epoch 5/30\n",
      "513920/513920 [==============================] - 32s 62us/sample - loss: 2.1392 - val_loss: 2.1378\n",
      "Epoch 6/30\n",
      "513920/513920 [==============================] - 31s 60us/sample - loss: 2.1378 - val_loss: 2.1378\n",
      "Epoch 7/30\n",
      "513920/513920 [==============================] - 28s 55us/sample - loss: 2.1368 - val_loss: 2.1365\n",
      "Epoch 8/30\n",
      "513920/513920 [==============================] - 32s 61us/sample - loss: 2.1363 - val_loss: 2.1366\n",
      "Epoch 9/30\n",
      "513920/513920 [==============================] - 32s 62us/sample - loss: 2.1356 - val_loss: 2.1349\n",
      "Epoch 10/30\n",
      "513920/513920 [==============================] - 32s 61us/sample - loss: 2.1352 - val_loss: 2.1355\n",
      "Epoch 11/30\n",
      "513920/513920 [==============================] - 30s 59us/sample - loss: 2.1349 - val_loss: 2.1352\n",
      "Epoch 12/30\n",
      "513920/513920 [==============================] - 31s 60us/sample - loss: 2.1346 - val_loss: 2.1355\n",
      "Epoch 13/30\n",
      "513920/513920 [==============================] - 31s 60us/sample - loss: 2.1343 - val_loss: 2.1345\n",
      "Epoch 14/30\n",
      "513920/513920 [==============================] - 31s 61us/sample - loss: 2.1341 - val_loss: 2.1360\n",
      "Epoch 15/30\n",
      "513920/513920 [==============================] - 36s 71us/sample - loss: 2.1339 - val_loss: 2.1354\n",
      "Epoch 16/30\n",
      "513920/513920 [==============================] - 30s 58us/sample - loss: 2.1337 - val_loss: 2.1355\n",
      "Epoch 17/30\n",
      "513920/513920 [==============================] - 29s 56us/sample - loss: 2.1335 - val_loss: 2.1349\n",
      "Epoch 18/30\n",
      "513920/513920 [==============================] - 28s 55us/sample - loss: 2.1334 - val_loss: 2.1345\n",
      "Epoch 19/30\n",
      "513920/513920 [==============================] - 28s 54us/sample - loss: 2.1333 - val_loss: 2.1351\n",
      "Epoch 20/30\n",
      "513920/513920 [==============================] - 28s 54us/sample - loss: 2.1331 - val_loss: 2.1350\n",
      "Epoch 21/30\n",
      "513920/513920 [==============================] - 27s 52us/sample - loss: 2.1331 - val_loss: 2.1355\n",
      "Epoch 22/30\n",
      "513920/513920 [==============================] - 26s 51us/sample - loss: 2.1330 - val_loss: 2.1355\n",
      "Epoch 23/30\n",
      "513920/513920 [==============================] - 29s 57us/sample - loss: 2.1329 - val_loss: 2.1360\n",
      "Epoch 24/30\n",
      "513920/513920 [==============================] - 27s 52us/sample - loss: 2.1329 - val_loss: 2.1356\n",
      "Epoch 25/30\n",
      "513920/513920 [==============================] - 29s 57us/sample - loss: 2.1327 - val_loss: 2.1353\n",
      "Epoch 26/30\n",
      "513920/513920 [==============================] - 28s 54us/sample - loss: 2.1327 - val_loss: 2.1351\n",
      "Epoch 27/30\n",
      "513920/513920 [==============================] - 32s 62us/sample - loss: 2.1326 - val_loss: 2.1359\n",
      "Epoch 28/30\n",
      "513920/513920 [==============================] - 28s 54us/sample - loss: 2.1325 - val_loss: 2.1355\n",
      "Epoch 29/30\n",
      "513920/513920 [==============================] - 28s 55us/sample - loss: 2.1325 - val_loss: 2.1367\n",
      "Epoch 30/30\n",
      "513920/513920 [==============================] - 29s 56us/sample - loss: 2.1324 - val_loss: 2.1370\n"
     ]
    }
   ],
   "source": [
    "history = flavormodel.fit(xf_train, yf_train, epochs=30, batch_size = BATCH_SIZE, validation_data=[xf_val,yf_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(513920, 89)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yf_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
